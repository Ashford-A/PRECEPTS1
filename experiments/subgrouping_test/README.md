# Testing the classification performance of oncogene mutation subgroupings #

In this experiment we train mutation classifiers to predict the presence of
subsets of genes' mutations in a tumor cohort. The characteristics of these
subgrouping classification models and especially their performances can then
be used to make inferences about the heterogeneity of downstream effects
caused by the mutations of a given cancer gene.

The results generated by this experiment form the basis of the paper

_"Systematic interrogation of mutation groupings reveals divergent downstream
expression programs within key cancer genes"_
**(MR Grzadkowski, HD Holly, J Somers, and E Demir)** 

[published](bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-021-04147-y)
in 2021 at BMC Bioinformatics. For the other analyses used in this paper, see
`../subgrouping_tour` and `../subgrouping_thresholds`, both of which follow an
analogous experiment structure and identical setup steps.


## Preparing to run the experiment ##

Clone the `dryads-research` git repository into the current directory:

```git clone https://github.com/ohsu-comp-bio/dryads-research.git```

Install the `research` conda environment, which will be activated by
the pipeline:

```conda env create -f environment.yml```

Register an account at www.synapse.org; use your credentials to
create a file named `~/.synapseConfig` with the following format:
```
[authentication]
username: ...
password: ...
```

Create a directory for Broad Firehose datasets, and then download the
expression data for TCGA tumor cohorts using the command:

```
firehose_get_latest/firehose_get -tasks RSEM_genes_normalized stddata latest
```

Add a file named `data_locs.py` under `dryads-research/experiments/utilities`.
This file will contain the locations of datasets downloaded from various
repositories. Some of these files are available through our Open Science
Framework data repository (osf.io/gr24t/).

The following variables must be added:

```
# where Broad Firehose datasets were downloaded
firehose_dir = "/home/users/datasets/..."

# the folder to use for the local Synapse cache
syn_root = "..."

# where the METABRIC datasets were downloaded from cBioPortal
metabric_dir = "..."

# a directory to be used by VEP to store genome datasets
vep_cache_dir = "..."

# available at www.oncokb.org/cancerGenes or osf.io/2m47r
oncogene_list = ".../OncoKB_cancerGeneList.txt"

# list of molecular subtypes identified by PCAWG in TCGA cohorts
# available at osf.io/2m47r/
subtype_file = ".../tcga_subtypes.txt"
```

Make sure you have the following environment variables defined in your bash
environment:

 - *$CODEDIR* where the `dryads-research` repo was checked out
 - *$TEMPDIR* a temporary location for intermediate experiment output files
 - *$DATADIR* a permanent location for the final experiment output files



## Running the experiment ##

Use `run_test.py` to launch the experiment pipeline. Depending on the compute
cluster setup you are using, you will need to modify `cluster.json` which by
default is designed for Slurm.

`run_test` first uses `setup_test.py` to enumerate the subgrouping tasks to be
tested. It then launches a batch of jobs on the configured compute cluster
using `Snakefile`, which in turn uses `fit_test.py` to tune, train, and test
the subgrouping classifiers, `gather_test.py` to process and consolidate
experiment output, and `merge_test.py` to concatenate processed output into
a single set of files for use by downstream analyses. The number of jobs in
the batch will depend on the number of enumerated tasks, the classifier used,
and the maximum time allowed for each `fit_test` job to run (1.5 days by
default).

Example usages of launching `run_test.py` in a Slurm compute environment
include:

 - running subgrouping tasks on the METABRIC(LumA) cohort using the
   Consequence->Exon mutation hierarchy, distributing subgrouping tasks among
   jobs submitted to the cluster such that each job takes no more than 250
   minutes:
```
 sbatch --mem-per-cpu=8000 -c 4 \
    --output=~/slurm_logs/subg-test.out --error=$slurm_dir/subg-test.err \
    dryads-research/experiments/subgrouping_test/run_test.sh \
    -e microarray -t METABRIC_LumA \
    -s 20 -l Consequence__Exon -c Ridge -m 250 -r
```

 - running subgrouping tasks on the TCGA-BLCA cohort downloaded from Firehose
   using a mutation hierarchy based on genomic location, distributing
   subgrouping tasks such that each job runs for a maximum of 10 hours:
```
 sbatch --mem-per-cpu=8000 -c 4 \
    --output=~/slurm_logs/subg-test.out --error=~/slurm_logs/subg-test.err \
    dryads-research/experiments/subgrouping_test/run_test.sh \
    -e Firehose -t BLCA -s 20 -l Exon__Position__HGVSp -c Ridge -m 600 -r
```

See `run_test.sh` for documentation on pipeline flags such as `-e` and `-l`.
Also note that depending on how your compute cluster is configured, you may
also need to use additional `sbatch` flags such as `--account` and `--exclude`
to submit the pipeline.

Because VEP, the mutation annotation tool used by `setup_test.py`, uses
threading, we recommend allocating four CPUs and plenty of memory to the
master pipeline job as shown. The threading behaviour of VEP can be adjusted
by editing the call to `process_variants` in
`dryads-research.features.cohorts.utils`.


## Analyzing the output of the experiment ##

Once an experiment is finished running, you should see output files saved in
the $DATADIR directory defined as above. Modules with names `plot_*.py` can be
used to generate plots of results generated from this output. Plots are by
default saved to the $DATADIR directory.

See `make_plots.sh` for examples of how to generate the set of plots used in
Grzadkowski et al.

