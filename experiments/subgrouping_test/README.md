# Testing the classification performance of oncogene mutation subgroupings #

In this experiment we train mutation classifiers to predict the presence of
subsets of genes' mutations in a tumor cohort. The characteristics of these
subgrouping classification models and especially their performances can then
be used to make inferences about the heterogeneity of downstream effects
caused by the mutations of a given cancer gene.

The results generated by this experiment form the basis of the paper

_"Systematic interrogation of mutation groupings reveals divergent downstream
expression programs within key cancer genes"_
**(MR Grzadkowski, HD Holly, J Somers, and E Demir)** 

[published](bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-021-04147-y)
in 2021 at BMC Bioinformatics. For the other analyses used in this paper, see
`../subgrouping_tour` and `../subgrouping_thresholds`, both of which follow an
analogous experiment structure and identical setup steps.


## Preparing to run the experiment ##

Clone the `dryads-research` git repository into the current directory:

```git clone https://github.com/ohsu-comp-bio/dryads-research.git```

Install the `research` conda environment, which will be activated by
the pipeline:

```conda env create -f environment.yml```

Register an account at www.synapse.org; use your credentials to
create a file named `~/.synapseConfig` with the following format:
```
[authentication]
username: ...
password: ...
```

Create a directory for Broad Firehose datasets, and then download the
expression data for TCGA tumor cohorts using the command:

```
firehose_get_latest/firehose_get -tasks RSEM_genes_normalized stddata latest
```

Add a file named `data_locs.py` under `dryads-research/experiments/utilities`.
This file will contain the locations of datasets downloaded from various
repositories. Some of these files are available through our Open Science
Framework data repository (osf.io/gr24t/).

The following variables must be added:

```
# where Broad Firehose datasets were downloaded
firehose_dir = "/home/users/datasets/..."

# the folder to use for the local Synapse cache
syn_root = "..."

# where the METABRIC datasets were downloaded from cBioPortal
metabric_dir = "..."

# a directory to be used by VEP to store genome datasets
vep_cache_dir = "..."

# available at www.oncokb.org/cancerGenes or osf.io/2m47r
oncogene_list = ".../OncoKB_cancerGeneList.txt"

# list of molecular subtypes identified by PCAWG in TCGA cohorts
# available at osf.io/2m47r/
subtype_file = ".../tcga_subtypes.txt"
```

Make sure you have the following environment variables defined in your bash
environment:

 - *$CODEDIR* where the `dryads-research` repo was checked out
 - *$TEMPDIR* a temporary location for intermediate experiment output files
 - *$DATADIR* a permanent location for the final experiment output files



## Running the experiment ##

Use `run_test.py` to launch the experiment pipeline. Depending on the compute
cluster setup you are using, you will need to modify `cluster.json` which by
default is designed for Slurm.

The arguments used by `run_test` are as follows:

- `expr_source` Where the expression data for the cohort was taken from.
                e.g. "Firehose" or "toil__gns" for TCGA cohorts, "microarray"
                for METABRIC.

- `cohort` The name of the tumor cohort.
           e.g. BRCA, LIHC, HNSC (TCGA cohorts); METABRIC, beatAML

- `samp_cutoff` The minimum number of samples a subgrouping must appear in
                to be considered by the experiment. The most commonly used
                value is 20 but higher values can be considered for large
                cohorts.

- `mut_levels` An ordered list of mutation annotation levels, separated by
               '__'. The four annotation level hierarchies used in the
               publication were thus "Consequence__Exon",
               "Pfam-domain__Consequence", "Exon__Position__HGVSp",
               and "SMART-domains__Consequence".

- `classif` The classifier that will be used to create subgrouping models.

- `test_max` The maximum number of minutes the pipeline should take to run
             to completion. The classification tasks will be parallelized
             across compute nodes in order to finish under this limit.

- `rewrite` Whether to delete existing intermediate output files for this
            experiment. Omitting this flag can be useful in cases where most
            but not all of the pipeline jobs finished in the first submission
            of the experiment; you can thus resubmit the `sbatch` command
            without the flag to have the pipeline only launch the missing jobs
            and avoid having to start from scratch.

- `count_only` Using this flag will only run the `setup_test` portion of the
               pipeline. This is useful for checking how many subgrouping
               tasks will be generated for a given cohort, sample cutoff, and
               list of mutation annotation levels before launching a
               potentially large number of cluster jobs.


`run_test` first uses `setup_test.py` to enumerate the subgrouping tasks to be
tested. It then launches a batch of jobs on the configured compute cluster
using `Snakefile`, which in turn uses `fit_test.py` to tune, train, and test
the subgrouping classifiers, `gather_test.py` to process and consolidate
experiment output, and `merge_test.py` to concatenate processed output into
a single set of files for use by downstream analyses. The number of jobs in
the batch will depend on the number of enumerated tasks, the classifier used,
and the maximum time allowed for each `fit_test` job to run (1.5 days by
default).

Example usages of launching `run_test.py` in a Slurm compute environment
include:

 - running subgrouping tasks on the METABRIC(LumA) cohort using the
   Consequence->Exon mutation hierarchy, distributing subgrouping tasks among
   jobs submitted to the cluster such that each job takes no more than 250
   minutes:
```
 sbatch --mem-per-cpu=8000 -c 4 \
    --output=~/slurm_logs/subg-test.out --error=$slurm_dir/subg-test.err \
    dryads-research/experiments/subgrouping_test/run_test.sh \
    -e microarray -t METABRIC_LumA \
    -s 20 -l Consequence__Exon -c Ridge -m 250 -r
```

 - running subgrouping tasks on the TCGA-BLCA cohort downloaded from Firehose
   using a mutation hierarchy based on genomic location, distributing
   subgrouping tasks such that each job runs for a maximum of 10 hours:
```
 sbatch --mem-per-cpu=8000 -c 4 \
    --output=~/slurm_logs/subg-test.out --error=~/slurm_logs/subg-test.err \
    dryads-research/experiments/subgrouping_test/run_test.sh \
    -e Firehose -t BLCA -s 20 -l Exon__Position__HGVSp -c Ridge -m 600 -r
```

Also note that depending on how your compute cluster is configured, you may
also need to use additional `sbatch` flags such as `--account` and `--exclude`
to submit the pipeline.

Because VEP, the mutation annotation tool used by `setup_test`, uses
threading, we recommend allocating four CPUs and plenty of memory to the
master pipeline job as shown. The threading behaviour of VEP can be adjusted
by editing the call to `process_variants` in
`dryads-research.features.cohorts.utils`.


## Intermediate data files generated by the experiment pipeline ##

This pipeline will store intermediate output for all experiments in an 
automatically created `$TEMPDIR/dryads-research/subgrouping_test/` directory.
This will contain the `setup` subdirectory, which stores cached cohort data
objects that are created once and then loaded by each subsequent run of the 
experiment as necessary. These objects have the names
`cohort-data__$expr_source__$cohort.p`, where `expr_source` matches the `-e`
argument used by the pipeline and `cohort` matches the `-t` argument. See
`features.cohorts.utils.load_cohort` for how these cohort objects are created
and managed.

The remaining subdirectories of the directory will each correspond to a single
run of the experiment, and have the structure
`$expr_source/$cohort__samps-$samp_cutoff/$mut_levels/$classif` corresponding
to the arguments used by the pipeline. Each such subdirectory will contain the
following files:

The remaining files of the form `out-*.p.gz` and `trnsf-preds.p.gz` are copies
of the final output files, which are described below.


## Final output files generated by the pipeline ##

See `gather_test.py` for how intermediate output files are consolidated into
the following final output files.

- `out-aucs.p.gz` The performance of each of the subgrouping tasks, measured
                  by taking the average samples scores across all ten cross-
                  validation folds (mean) as well as for each of the folds
                  individually (CV) and from concatenating the scores instead
                  of averaging them (all). In each case AUCs are calculated
                  by calculating the probability that a sample mutated for the
                  subgrouping is assigned a higher score than a sample 
                  wild-type for the subgrouping.

- `out-conf.p.gz` Bootstrapped AUCs calculated by using the "mean" method
                  described above on a randomly chosen subset of samples. This
                  process is repeated a thousand times, each time using a
                  subset of samples created by choosing each sample in the
                  cohort with probability 0.5. For each subgrouping task this
                  thus creates a population of AUCs that provides a confidence
                  interval around the AUC calculated using all samples. These
                  AUCs were originally used to compare different task AUCs to
                  one another in our publication, but were eventually replaced
                  by cv-significance and could be deprecated in the future.

- `out-pheno.p.gz` The task labels for each subgrouping, i.e. binary vectors
                   indicating whether each cohort sample carries a mutation in
                   the subgrouping or not. The order of samples in these
                   vectors is the same as in the matrix of predicted scores
                   stored in `out-pred`.

- `out-pred.p.gz` The predicted scores made by each subgrouping task for each
                  sample. There are ten scores for each combination of sample
                  and task, one for each cross-validation iteration.

- `out-coef.p.gz` The model coefficients for each subgrouping task. Each gene
                  feature used by the model thus has forty different
                  coefficents, one for each cross-validation fold and 
                  iteration.

- `out-tune.p.gz` Tuning accuracies and other information as collected during
                  the subgrouping model training process. This is given as
                  three matrices, the first giving the optimal hyper-parameter
                  value chosen by each iteration of each model, the second
                  listing the average and the standard deviations of the times
                  taken to train each model for each hyper-parameter value, 
                  and the third giving the average of tuning accuracies for
                  each tested hyper-parameter value. The final entry is the
                  classifier class used by this experiment, which can be used
                  to check e.g. the algorithm parameters used.
                  See `plot_tuning.py` for examples of how to use this data
                  structure to draw insights regarding the behaviour of a
                  classification algorithm across subgrouping tasks.


## Analyzing the output of the experiment ##

Once an experiment is finished running, you should see output files saved in
the $DATADIR directory defined as above. Modules with names `plot_*.py` can be
used to generate plots of results generated from this output. Plots are by
default saved to the $DATADIR directory.

See `make_plots.sh` for examples of how to generate the set of plots used in
Grzadkowski et al.

